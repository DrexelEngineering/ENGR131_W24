{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"Final_Exam_Practice.ipynb\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# ⌛️ Practice Final Exam\n",
    "\n",
    "This Practice Final Exam is designed to test your mastery of the Python programming language. You will be asked to respond to a series of questions both multiple choice and short answer problems using python. 10% of the points earned on this practice final prior to the exam will be added to your final exam score. \n",
    "\n",
    "\n",
    "## Instructions\n",
    "\n",
    "You are allowed to use anything with the python kernel to help you solve the problems. You must run all code and press the submit button when shown to record your grades. If you do not do this your scores might not be recorded. All of your responses should be persistent if you restart the kernel, however entries can only be updated if you run the cell and hit submit. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Part 1: Entering Your Information for Credit\n",
    "\n",
    "To receive credit for assignments, it is important we can identify your work from others. To do this, we will ask you to enter your information in the following code block.\n",
    "\n",
    "You must hit submit and see the message saying the data is saved. If you do not see this message contact your instructor to ensure you are getting credit for your work.\n",
    "\n",
    "**Note: do not delete the output.log file or the .responses.json file or you might lose your current work.**\n",
    "\n",
    "### Before you begin\n",
    "\n",
    "Run the block of code at the top of the notebook that imports and sets up the autograder. This will allow you to check your work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "from subprocess import call\n",
    "import sys\n",
    "\n",
    "package_name = 'ENGR131_Util_2024'\n",
    "version = '1.0.0'\n",
    "package_version = f'{package_name}=={version}'\n",
    "\n",
    "try:\n",
    "    # Check if the package and version are installed\n",
    "    pkg_resources.require(package_version)\n",
    "    print(f'{package_version} is already installed.')\n",
    "except pkg_resources.DistributionNotFound:\n",
    "    # If not installed, install the package\n",
    "    print(f'{package_version} not found. Installing...')\n",
    "    call([sys.executable, '-m', 'pip', 'install', package_version])\n",
    "except pkg_resources.VersionConflict:\n",
    "    # If a different version is installed, you can choose to upgrade/downgrade\n",
    "    installed_packages = {dist.key: dist.version for dist in pkg_resources.working_set}\n",
    "    installed_version = installed_packages.get(package_name.lower())\n",
    "    print(f'{package_name} {installed_version} is installed, but {version} is required.')\n",
    "    # Optionally, upgrade or downgrade the package to the required version\n",
    "    call([sys.executable, '-m', 'pip', 'install', '--upgrade', package_version])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "from ENGR131_Util_2024 import responses, StudentInfoForm, cell_logger\n",
    "from ENGR131_Util_2024 import submit_question, ResponseStore, ValidateLogFile\n",
    "\n",
    "\n",
    "# Register the log function to be called before any cell is executed\n",
    "get_ipython().events.register('pre_run_cell', cell_logger)\n",
    "responses[\"assignment\"] = \"practicefinal_1\"\n",
    "\n",
    "StudentInfoForm(**responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "**Question 1: Match the Correct Definition**\n",
    "\n",
    "*18 points*\n",
    "\n",
    "Please run the block of code below and respond to the prompts with the most suitable response. \n",
    "\n",
    "Make sure you hit the submit button or your grade will not be recorded. Confirmation that your responses are recorded will appear below the code block. We will only grade your most recent set of responses. You can submit entries as many times as you like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# from ENGR131_Util_2024 import DataTypes\n",
    "from ENGR131_Util_2024 import responses\n",
    "from ENGR131_Util_2024 import TypesQuestion\n",
    "\n",
    "TypesQuestion(**responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "**Question 2: Select the option that matches the definition** \n",
    "\n",
    "*9 points*\n",
    "\n",
    "Please run the block of code below and respond to the prompts with the most suitable response. \n",
    "\n",
    "Make sure you hit the submit button or your grade will not be recorded. Confirmation that your responses are recorded will appear below the code block. We will only grade your most recent set of responses. You can submit entries as many times as you like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "from ENGR131_Util_2024 import MCQQuestion\n",
    "from ENGR131_Util_2024 import responses\n",
    "\n",
    "MCQQuestion(**responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "**Question 3: Select all statements which are TRUE** \n",
    "\n",
    "*20 points*\n",
    "\n",
    "Please run the block of code below and respond to the prompts with the most suitable response. \n",
    "\n",
    "Make sure you hit the submit button or your grade will not be recorded. Confirmation that your responses are recorded will appear below the code block. We will only grade your most recent set of responses. You can submit entries as many times as you like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "from ENGR131_Util_2024 import responses\n",
    "from ENGR131_Util_2024 import SelectMany\n",
    "\n",
    "\n",
    "SelectMany(**responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "**Question 4: Reading and Interpreting Python Code** \n",
    "\n",
    "*50 points*\n",
    "\n",
    "You are provided with the following code:\n",
    "\n",
    "```python\n",
    "1   numbers = [5, 2, 3.0]\n",
    "2   even = 0\n",
    "3   odd = 0\n",
    "4\n",
    "5   while odd < 6:\n",
    "6       for num in numbers:\n",
    "7           if num % 2 == 0:\n",
    "8               even += num\n",
    "9           else:\n",
    "10              odd += num\n",
    "```\n",
    "\n",
    "Please run the block of code below and respond to the prompts with the most suitable response. \n",
    "\n",
    "Make sure you hit the submit button or your grade will not be recorded. Confirmation that your responses are recorded will appear below the code block. We will only grade your most recent set of responses. You can submit entries as many times as you like. \n",
    "\n",
    "1. You will be asked to select the most relevant comment for the lines of code selected. \n",
    "2. You will be asked to describe how the code is executed. Please use the debugger tool to help ensure you have a correct answer.\n",
    "\n",
    "   1. Line number: This is the line number of the code that is run during the execution of the code.\n",
    "   2. Variable Changed: This is the variable whose value is changed during the execution of the code. *Note* if no variable is changed - such as when evaluating logical expressions, select \"None\".\n",
    "   3. Current Value: This is the value of the variable after the line of code is executed. If a logical expression is evaluated, please enter the value of the logical expression.\n",
    "   4. DataType: This is the data type of the variable after the line of code is executed. If a logical expression is evaluated, please enter the data type of the logical expression.\n",
    "   \n",
    "**Note**: The autograder has mechanisms to provide partial credit. If you skip a step, it will continue grading at the next correct step. \n",
    "\n",
    "We provide the code in Python for your convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "numbers = [5, 2, 3.0]\n",
    "even = 0\n",
    "odd = 0\n",
    "\n",
    "while odd < 6:\n",
    "   for num in numbers:\n",
    "      if num % 2 == 0:\n",
    "         even += num\n",
    "      else:\n",
    "         odd += num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "from ENGR131_Util_2024 import ReadingPythonQuestion\n",
    "from ENGR131_Util_2024 import responses\n",
    "\n",
    "ReadingPythonQuestion(**responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 5: Making a Graph \n",
    "\n",
    "*35 points*\n",
    "\n",
    "A friend gives you a card with the following function written on it. You need to figure out what they are saying. Implement a function to reveal the hidden message.\n",
    "\n",
    "1. Import the packages you need.\n",
    "\n",
    "   - You will need `numpy` import this as the variable `np`\n",
    "   - You will need the `pyplot` module from `matplotlib`. Import `pyplot` as a variable `plt`.\n",
    "\n",
    "\n",
    "2. Make a function called `draw_card_eq`.\n",
    "   - This should take one input `a`.\n",
    "   - Create a linearly-spaced vector using the `linspace` method in `np`.\n",
    "     - The vector should go from 0 to $2\\pi$ with 2000 steps.\n",
    "     - Assign this vector to the variable `t`.\n",
    "   - Write the equation for `x`:\n",
    "     $$ x = a(16(\\sin(t))^3)$$\n",
    "   - Write the equation for `y`:\n",
    "     $$ y = a(13\\cos(t) - 5\\cos(2t) - 2\\cos(3t) - \\cos(4t))$$\n",
    "\n",
    "   - Use the `plot` method in `plt` to plot `x` vs `y`, set the plot color to `red` using the input `red` as a string\n",
    "     - Save this to the variable `plot_`.\n",
    "   - Return `plot_`.\n",
    "\n",
    "3. Call the function `draw_card_eq` with `a` equal to 1. Doing so will reveal the message. \n",
    "   - Save the output of the function to a variable `plot_`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# Your import statements go here.\n",
    "# We would recommend checking this with the autograder.\n",
    "# Completion of this step should pass tests 1-2.\n",
    "...\n",
    "\n",
    "# Your function goes here\n",
    "# We would recommend checking this with the autograder.\n",
    "# Completion of this step should pass test 3.\n",
    "...\n",
    "\n",
    "# Call your function here\n",
    "# Make sure your check your code with the autograder. If you do not run the autograder your responses will not be graded.\n",
    "# Completion of this step pass all tests.\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5 - Making a Graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Recommended\n",
    "\n",
    "We recommend that you run the following code to ensure your responses are recorded. This is an extra measure to ensure your results are aaved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "submit_question()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 6: Using Data Types (30 Points)\n",
    "\n",
    "We have written a function that converts a DataType. Pass values with the expected DataTypes as defined below to the `convert_to_string` function. The function will print the output. \n",
    "\n",
    "0. Run the `convert_to_string` function first.\n",
    "1. Pass an integer `42` to `convert_to_string`. \n",
    "    - Save the output to a variable `int_`.\n",
    "2. Pass an float `3.14159` to `convert_to_string`. \n",
    "    - Save the output to a variable `float_`.\n",
    "3. Pass an boolean with a true value to `convert_to_string`. \n",
    "    - Save the output to a variable `bool_`.\n",
    "4. Pass an string `I love coding` to `convert_to_string`. \n",
    "    - Save the output to a variable `string_`.\n",
    "5. Pass a list of integers `1`, `2`, `3` to `convert_to_string`. \n",
    "    - Save the output to a variable `list_`.\n",
    "6. Pass a tuple of values `a`, `b`, `c` to `convert_to_string`. \n",
    "    - Save the output to a variable `tuple_`.\n",
    "7. Pass a dictionary with a key `name` and a value `jay` as a string and a key `age` and a value `12` as an integer to `convert_to_string`. \n",
    "    - Save the output to a variable `dict_`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "def convert_to_string(data):\n",
    "    if isinstance(data, int):\n",
    "        print(data)\n",
    "    elif isinstance(data, float):\n",
    "        print(\"{:.2f}\".format(data))\n",
    "    elif isinstance(data, bool):\n",
    "        print(str(data))\n",
    "    elif isinstance(data, str):\n",
    "        print(data)\n",
    "    elif isinstance(data, list):\n",
    "        elements = \", \".join([str(x) for x in data])\n",
    "        print(elements)\n",
    "    elif isinstance(data, tuple):\n",
    "        elements = \", \".join([str(x) for x in data])\n",
    "        print(elements)\n",
    "    elif isinstance(data, dict):\n",
    "        pairs = \", \".join(\n",
    "            [\"Key = \" + str(k) + \" Value = \" + str(v) for k, v in data.items()]\n",
    "        )\n",
    "        print(pairs)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# 1. Pass an integer `42` to `convert_to_string`. \n",
    "# Save the output to a variable `int_`.\n",
    "# completion of this step will pass test 1\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# 2. Pass an float `3.14159` to `convert_to_string`. \n",
    "# Save the output to a variable `float_`.\n",
    "# completion of this step will pass test 2\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# 3. Pass an boolean with a true value to `convert_to_string`. \n",
    "# Save the output to a variable `bool_`.\n",
    "# completion of this step will pass test 3\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# 4. Pass an string `I love coding` to `convert_to_string`. \n",
    "# Save the output to a variable `string_`.\n",
    "# completion of this step will pass test 4\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# 5. Pass a list of integers `1`, `2`, `3` to `convert_to_string`. \n",
    "# Save the output to a variable `list_`.\n",
    "# completion of this step will pass test 5\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# 6. Pass a tuple of values `a`, `b`, `c` to `convert_to_string`. \n",
    "# Save the output to a variable `tuple_`.\n",
    "# completion of this step will pass test 6\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# 7. Pass a dictionary with a key `name` and a value `jay` as a string and a key `age` and a value `12` as an integer to `convert_to_string`. \n",
    "# Save the output to a variable `dict_`.\n",
    "# completion of this step will pass test 7\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6 - Exploring DataTypes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Recommended\n",
    "\n",
    "We recommend that you run the following code to ensure your responses are recorded. This is an extra measure to ensure your results are aaved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "submit_question()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 7: Building a Thermometer (36 Points)\n",
    "\n",
    "In chemical engineering, it is common to monitor the temperature of a reactor. We will build a class to implement a temperature sensor. This sensor will save and also set a temperature value in Celsius. Additionally, the class will convert the temperature given in Celsius to Fahrenheit.\n",
    "\n",
    "1. Build a class called `Thermometer`. This class will have to be initialized and have two methods.\n",
    "    - Add an initialization method that accepts an initial temperature in Celsius as a variable `temperature`.\n",
    "      - Save the `temperature` as an object attribute `temperature_c`. This is the temperature in $^{\\circ}C$ .\n",
    "\n",
    "    - Build a method `set_temperature` that will set the object attribute `temperature_c`.\n",
    "      - This method takes one input `temperature`.\n",
    "      - We want to make sure the input is a number. Do this by **trying** to convert it to a `float` using the built in `float` method.\n",
    "        - If `temperature` is a number that can be converted to a float, save it to the object attribute `temperature_c`.\n",
    "       - If `temperature` cannot be converted to a float, set the object attribute `temperature_c` to the string 'fail'.\n",
    "\n",
    "    - Build a method `to_fahrenheit` that converts the temperature from Celsius to Fahrenheit.\n",
    "      - Convert the temperature using the following equation.\n",
    "        $$Fahrenheit = \\frac{9}{5}Celsius +32 $$\n",
    "      - Return the temperature in Fahrenheit.  \n",
    "\n",
    "2. Instantiate a `Thermometer` `water_sensor` with an initial `temperature` of `12` $^{\\circ}C$.\n",
    "   \n",
    "3. Extract the temperature in $^{\\circ}C$ from the object `water_sensor`.\n",
    "    - Assign the output to a variable `temp_c`.\n",
    "\n",
    "4. Extract the temperature in $^{\\circ}F$ from the object `water_sensor`.\n",
    "    - Assign the output to a variable `temp_f`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# Your class for Thermometer goes here\n",
    "...\n",
    "\n",
    "# Instantiate the Thermometer water_sensor here\n",
    "...\n",
    "\n",
    "# extract the temperature in degrees celsius\n",
    "...\n",
    "\n",
    "# extract the temperature in degrees fahrenheit\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "grader.check(\"q7-Building a Thermometer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Recommended\n",
    "\n",
    "We recommend that you run the following code to ensure your responses are recorded. This is an extra measure to ensure your results are aaved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "submit_question()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 8: Determining a Price for a Circuit (40 Points)\n",
    "\n",
    "As an engineer, you might want to estimate the total price at which you would sell a circuit you designed based on the cost of goods from a supplier and a desired profit margin. We will write a Python function to assist you in this task.\n",
    "\n",
    "We have provided you with two dictionaries, `parts_cost` and `circuit_parts`. `parts_cost` defines the the parts from the supplier, and `circuit_parts` defines the components needed to build the circuit. The circuit uses two `Custom Processor` components, which are not available from the supplier, that each costs $12.\n",
    "\n",
    "*Cost of Goods from Supplier*\n",
    "\n",
    "| Part Name (Key)     | Cost per Item (value, in $) |\n",
    "| ---------------- | -------------- |\n",
    "| 10-ohm resistor  | 0.025          |\n",
    "| 100-ohm resistor | 0.03           |\n",
    "| 1-pF capacitor   | 0.05           |\n",
    "| 10-pF capacitor  | 0.12           |\n",
    "\n",
    "*Components Required for Building the Circuit*\n",
    "\n",
    "| Part Name  (Key)       | Quantity (sub-key) | Cost to Build (value, in $)|\n",
    "| ---------------- | -------- | -------------- |\n",
    "| 10-ohm resistor  | 10       | N/A             |\n",
    "| 10-pF capacitor  | 3        | N/A            |\n",
    "| 1-pF capacitor   | 7        | N/A            |\n",
    "| Custom Processor | 2        | 12            |\n",
    "\n",
    "1. Import the package `numpy` as `np`. \n",
    "    - You might not need to do this step if you have done it as part of the previous questions; however, it is good practice.\n",
    "2. Define a function `price_calculator` that accepts three inputs: `parts_cost`, `circuit_parts`, and `profit`.\n",
    "    - Initialize a variable `cost_of_goods`, and set it equal to 0.\n",
    "    - Using a loop, compute the total `cost_of_goods`. \n",
    "        - You should use the `.items()` method of a dictionary that returns an iterator, where each iteration is a tuple containing the key and the value. \n",
    "        - You will need to use a branching statement to get the cost per item from the dictionary with the cost per item or the nested dictionary with the cost to build, whichever is appropriate for the part.\n",
    "    - Determine the `sales_price` by adding the profit to the `cost_of_goods`. \n",
    "        - Multiply the `cost_of_goods` by (1 + `profit`). \n",
    "        - Using the `np.round` method, round the price to 2 decimal places. \n",
    "        - Save this value to the variable `sales_price`.\n",
    "    - Call the `print_total` function within `price_calculator`.\n",
    "        - Save the output to the variable `order_total`.\n",
    "    - Print the variable `order_total` within the `price_calculator` function.\n",
    "    - Return the variable `sales_price`.\n",
    "3. Call the function `price_calculator` with the inputs `parts_cost`, `circuit_parts`, `profit`. \n",
    "    - Save the output to the variable `total_price`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# Your import statement goes here.\n",
    "...\n",
    "\n",
    "parts_cost = {\n",
    "    \"10-ohm resistor\": 0.025,\n",
    "    \"100-ohm resistor\": 0.03,\n",
    "    \"1-pF capacitor\": 0.05,\n",
    "    \"10-pF capacitor\": 0.12,\n",
    "}\n",
    "\n",
    "circuit_parts = {\n",
    "    \"10-ohm resistor\": {\"quantity\": 10},\n",
    "    \"10-pF capacitor\": {\"quantity\": 3},\n",
    "    \"1-pF capacitor\": {\"quantity\": 7},\n",
    "    \"Custom Processor\": {\"quantity\": 2, \"price\": 12},\n",
    "}\n",
    "\n",
    "# profit assignment goes here, we have set this to 63%\n",
    "profit = 0.63\n",
    "\n",
    "# We have provided you with a function to print the total cost of the order\n",
    "# you need to call this function but do not need to change it.\n",
    "def print_total(sales_price):\n",
    "    return f\"Total cost of the order: ${sales_price}\"\n",
    "\n",
    "\n",
    "# Your function price_calculator goes here\n",
    "# Completion of this step should pass test 3\n",
    "...\n",
    "\n",
    "# Your call of the function price_calculator goes here\n",
    "# Completion of this step should pass all tests\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "grader.check(\"q8-Determining a Price for a Circuit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Recommended\n",
    "\n",
    "We recommend that you run the following code to ensure your responses are recorded. This is an extra measure to ensure your results are aaved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "submit_question()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 9: Plotting and Fitting (60 Points):\n",
    "\n",
    "One of the most common tasks in engineering is fitting data to a model. For this question we will generate some noisy data with a gaussian distribution - one of the most common statistical distributions. We will then plot this data, and fit this data to the model.\n",
    "\n",
    "1. Start by importing the necessary tools. If you imported any of these packages before, you might not need to do so again; however, it is good practice.\n",
    "\n",
    "   - Import `numpy` as the variable `np`.\n",
    "   - Import the `pyplot` module from `matplotlib` as a variable `plt`. \n",
    "   - Import the function `curve_fit` from `scipy.optimize`.\n",
    "\n",
    "2. Define a function `gaussian` that computes the Gaussian function using the following equation:\n",
    "   $$Ae^{\\frac{-(x-b)^2}{2c^2}}$$\n",
    "   - The function should accept inputs of `x`, `a`, `b`, and `c`.\n",
    "\n",
    "3. Use the `linspace` method in `numpy` to generate a vector from `-20` to `20` with `100` steps. \n",
    "   - Save this vector to the variable `x_data`.\n",
    "\n",
    "4. Use the defined `gaussian` function to compute the true, resulting value when `a` is `3`, `b` is `4`, and `c` is `7`. \n",
    "   - Save this to the variable `y_true`.\n",
    "\n",
    "5. Generate noisy data using the `random.normal` method in `numpy`. \n",
    "   - The noisy data should be the same size as the `x_data`. \n",
    "   - Scale the noise by a factor of `0.3`, and add the noise the `y_true` variable.\n",
    "   - Assign the output to `y_noisy`.\n",
    "\n",
    "6. Using the `scatter` method from `matplotlib` `pyplot`, plot the `x_data` vs `y_noisy`. \n",
    "   - Add a label, \"Noisy Data\". \n",
    "   - Save this plot to the variable `plot_1`.\n",
    "\n",
    "7. Use the imported `curve_fit` method to fit the noisy data to the `gaussian` function over the range of `x_data`. \n",
    "   - Save the fit results to the variable `popt`, and the parameter covariances to a variable `pcov`. \n",
    "     - **Do not provide any initial guess**, or provide `None`. If you provide an initial guess, the autograder might fail.\n",
    "\n",
    "8. Calculate the fitted curve using the defined `gaussian` function. \n",
    "   - You can use the \\* method to unpack the list of fitted parameters if you like. \n",
    "   - Save the result to a variable `y_fit`.\n",
    "\n",
    "9.  Plot the fit results as `x_data` vs `y_fit` using the `plot` method in `pyplot`. \n",
    "    - Set the color of the line to green using the `g` tag.\n",
    "    - Add a label, \"Fitted Gaussian\". \n",
    "    - Save this plot to the variable `plot_2`.\n",
    "\n",
    "10. Using the methods within `plt`, set the `x_label` and `y_label` to `x` and `y`, respectively.\n",
    "    \n",
    "11. Use the `legend` method in `plt` to add a legend.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# 1. Add your import statements here. \n",
    "# Completion of this step will pass the first test\n",
    "...\n",
    "\n",
    "# do not delete the line of code below, it will cause problems with the autograder.\n",
    "np.random.seed(42)\n",
    "\n",
    "# 2. Define the Gaussian function.\n",
    "# completion of this step will pass test 2.\n",
    "...\n",
    "\n",
    "# 3. define your x_data vector\n",
    "# completion of this step will pass test 3.\n",
    "...\n",
    "\n",
    "# 4. Call the gaussian function as described above\n",
    "# save the output to the variable y_true\n",
    "# completion of this step will pass test 4.\n",
    "...\n",
    "\n",
    "#5. Generate the noisy data described above\n",
    "# save the output to the variable y_noisy\n",
    "# completion of this step will pass test 5.\n",
    "...\n",
    "\n",
    "# 6. Plot the noisy data using a scatterplot\n",
    "# make sure to add the label \"Noisy Data\"\n",
    "# save the output to the variable plot_1\n",
    "...\n",
    "\n",
    "# 7. Fit the noisy data to a Gaussian function using curve_fit\n",
    "# save the outputs to the variables popt and pcov\n",
    "# completion of this step will pass test 6.\n",
    "...\n",
    "\n",
    "# 8. Calculate the fitted curve using the gaussian function\n",
    "# save the output to the variable y_fit\n",
    "# completion of this step will pass test 7. \n",
    "...\n",
    "\n",
    "# 9. Plot the fitted results as described in the instructions\n",
    "# save the output to the variable plot_2\n",
    "# completion of this step will pass several more tests. \n",
    "# make sure to add the label \"Fitted Gaussian\"\n",
    "...\n",
    "\n",
    "# 10. Add axis labels to the plot\n",
    "# completion of this step will pass a test.\n",
    "...\n",
    "\n",
    "# 11. Add a legend to the plot\n",
    "# completion of this step will pass all tests.\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "grader.check(\"q9-Plotting and Fitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Recommended\n",
    "\n",
    "We recommend that you run the following code to ensure your responses are recorded. This is an extra measure to ensure your results are aaved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "submit_question()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Validating your log file\n",
    "\n",
    "Before your submit your results we would recommend running the following command to validate your log file. This will ensure that you have followed the correct format and that your log file is valid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "ValidateLogFile(\"./output.log\", responses[\"assignment\"], 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Submitting Your Assignment\n",
    "\n",
    "To submit your assignment please use the following link the assignment on GitHub classroom.\n",
    "   \n",
    "Use this [link](https://classroom.github.com/a/ClDJl_RS) to navigate to the assignment on GitHub classroom.\n",
    "\n",
    "**Please submit just your output.log file. No other files are needed.**\n",
    "\n",
    "\n",
    "If you need further instructions on submitting your assignment, please ask your TA during lab. \n",
    "\n",
    "## Viewing your score\n",
    "\n",
    "Each `log` file you have uploaded will have a file with the name of your file + `Grade_Report.md`. You can view this file by clicking on the file name. This will show you the results of the autograder. \n",
    "\n",
    "**Make sure you run your code, and run the tests. If you do not run the test you will not get credit for your work.**\n",
    "\n",
    "```{note}\n",
    "In python and particularly jupyter notebooks it is common that during testing you run cells in a different order, or run cells and modify them. This can cause there to be local variables needed for your solution that would not be recreated on running your code again from scratch. Your assignment will be graded based on running your code from scratch. This means before you submit your assignment you should restart the kernel and run all cells. You can do this by clicking `Kernel` and selecting `Restart and Run All`. If you code does not run as expected after restarting the kernel and running all cells it means you have an error in your code. \n",
    "```\n",
    "\n",
    "## Fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q5 - Making a Graph": {
     "name": "q5 - Making a Graph",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> import drexel_jupyter_logger\n>>> from ENGR131_Util_2024 import submit_score\n>>> from unittest.mock import patch\n>>> import json\n>>> from ENGR131_Util_2024 import responses, ResponseStore\n>>> responder = ResponseStore()\n>>> responder.create_file()\n>>> points_ = [3, 3, 3, 6, 14, 6]\n>>> for (i, point) in enumerate(points_):\n...     drexel_jupyter_logger.variable_logger_csv(f'0, {point}', f'q5_{i + 1}')\n>>> if 'drexel_email' not in responses:\n...     raise ValueError('Please fill out the student info form and run the test again')\n>>> scorer = submit_score()\n>>> question_id = 'q5_1'\n>>> max_score = 3\n>>> score = 0\n>>> if np.__version__ is not None:\n...     score = max_score\n>>> drexel_jupyter_logger.variable_logger_csv(f'{score}, {max_score}', question_id)\n>>> response = {'question_id': question_id, 'score': score, 'max_score': max_score, 'student_response': np.__version__}\n>>> responder.add_response(response)\n>>> assert np.__version__ is not None, 'numpy is incorrectly imported'\n",
         "failure_message": "numpy incorrectly implemented.",
         "hidden": false,
         "locked": false,
         "points": 3,
         "success_message": "numpy correctly implemented."
        },
        {
         "code": ">>> import drexel_jupyter_logger\n>>> from ENGR131_Util_2024 import submit_score\n>>> from unittest.mock import patch\n>>> import numpy as np\n>>> import json\n>>> from ENGR131_Util_2024 import responses, ResponseStore\n>>> responder = ResponseStore()\n>>> if 'drexel_email' not in responses:\n...     raise ValueError('Please fill out the student info form and run the test again')\n>>> scorer = submit_score()\n>>> question_id = 'q5_2'\n>>> max_score = 3\n>>> score = 0\n>>> if plt.__package__ is not None:\n...     score = max_score\n>>> drexel_jupyter_logger.variable_logger_csv(f'{score}, {max_score}', question_id)\n>>> response = {'question_id': question_id, 'score': score, 'max_score': max_score, 'student_response': plt.__package__}\n>>> responder.add_response(response)\n>>> assert plt.__package__ is not None, 'plt is incorrectly imported'\n",
         "failure_message": "plt random incorrectly implemented.",
         "hidden": false,
         "locked": false,
         "points": 3,
         "success_message": "plt correctly implemented."
        },
        {
         "code": ">>> import drexel_jupyter_logger\n>>> from ENGR131_Util_2024 import submit_score\n>>> from unittest.mock import patch\n>>> import numpy as np\n>>> from ENGR131_Util_2024 import responses\n>>> import json\n>>> from ENGR131_Util_2024 import responses, ResponseStore\n>>> responder = ResponseStore()\n>>> if 'drexel_email' not in responses:\n...     raise ValueError('Please fill out the student info form and run the test again')\n>>> scorer = submit_score()\n>>> question_id = 'q5_3'\n>>> max_score = 3\n>>> score = 0\n>>> if callable(draw_card_eq):\n...     score = max_score\n>>> drexel_jupyter_logger.variable_logger_csv(f'{score}, {max_score}', question_id)\n>>> response = {'question_id': question_id, 'score': score, 'max_score': max_score, 'student_response': callable(draw_card_eq)}\n>>> responder.add_response(response)\n>>> assert callable(draw_card_eq), 'draw_card_eq is not a function'\n",
         "failure_message": "draw_card_eq is not a function",
         "hidden": false,
         "locked": false,
         "points": 3,
         "success_message": "draw_card_eq is callable."
        },
        {
         "code": ">>> import drexel_jupyter_logger\n>>> from ENGR131_Util_2024 import submit_score\n>>> from unittest.mock import patch\n>>> import numpy as np\n>>> import json\n>>> from ENGR131_Util_2024 import responses, ResponseStore\n>>> import matplotlib.pyplot as plts\n>>> responder = ResponseStore()\n>>> if 'drexel_email' not in responses:\n...     raise ValueError('Please fill out the student info form and run the test again')\n>>> scorer = submit_score()\n>>> question_id = 'q5_4'\n>>> max_score = 6\n>>> score = 0\n>>> if isinstance(plot_[0], plts.Line2D):\n...     score = max_score\n>>> drexel_jupyter_logger.variable_logger_csv(f'{score}, {max_score}', question_id)\n>>> response = {'question_id': question_id, 'score': score, 'max_score': max_score, 'student_response': isinstance(plot_[0], plts.Line2D)}\n>>> scorer.add_response(response)\n>>> with patch('builtins.print') as mock_print:\n...     scorer.submit()\n>>> scorer = submit_score()\n>>> assert isinstance(plot_[0], plts.Line2D), 'You did not return a plot object.'\n",
         "failure_message": "You did not return a plot object.",
         "hidden": false,
         "locked": false,
         "points": 6,
         "success_message": "You successfully returned a plot object."
        },
        {
         "code": ">>> import drexel_jupyter_logger\n>>> from ENGR131_Util_2024 import submit_score\n>>> from unittest.mock import patch\n>>> import numpy as np\n>>> import json\n>>> from ENGR131_Util_2024 import responses, ResponseStore\n>>> responder = ResponseStore()\n>>> if 'drexel_email' not in responses:\n...     raise ValueError('Please fill out the student info form and run the test again')\n>>> scorer = submit_score()\n>>> question_id = 'q5_5'\n>>> max_score = 14\n>>> score = 0\n>>> if np.isclose(sum(plot_[0].get_xdata()[::3]), 1.6561871562318847e-07):\n...     score += int(max_score / 2)\n>>> if np.isclose(sum(plot_[0].get_ydata()[::4]), 1.2500506348082654):\n...     score += int(max_score / 2)\n>>> drexel_jupyter_logger.variable_logger_csv(f'{score}, {max_score}', question_id)\n>>> response = {'question_id': question_id, 'score': score, 'max_score': max_score, 'student_response': sum(plot_[0].get_xdata()[::3])}\n>>> responder.add_response(response)\n>>> assert np.isclose(sum(plot_[0].get_xdata()[::3]), 1.6561871562318847e-07), 'The values for x_data in the plot are incorrect. '\n>>> assert np.isclose(sum(plot_[0].get_ydata()[::4]), 1.2500506348082654), 'The values for y_data in the plot are incorrect. '\n",
         "failure_message": "The values for your plot are incorrect.",
         "hidden": false,
         "locked": false,
         "points": 14,
         "success_message": "The values for your plot are correct."
        },
        {
         "code": ">>> import drexel_jupyter_logger\n>>> from ENGR131_Util_2024 import submit_score\n>>> from unittest.mock import patch\n>>> import numpy as np\n>>> import json\n>>> from ENGR131_Util_2024 import responses, ResponseStore\n>>> responder = ResponseStore()\n>>> if 'drexel_email' not in responses:\n...     raise ValueError('Please fill out the student info form and run the test again')\n>>> scorer = submit_score()\n>>> question_id = 'q5_6'\n>>> max_score = 6\n>>> score = 0\n>>> if plot_[0].get_color() == 'red' or plot_[0].get_color() == (1.0, 0.0, 0.0, 1.0) or plot_[0].get_color() == 'r':\n...     score = max_score\n>>> drexel_jupyter_logger.variable_logger_csv(f'{score}, {max_score}', question_id)\n>>> response = {'question_id': question_id, 'score': score, 'max_score': max_score, 'student_response': plot_[0].get_color()}\n>>> responder.add_response(response)\n>>> assert plot_[0].get_color() == 'red' or plot_[0].get_color() == (1.0, 0.0, 0.0, 1.0) or plot_[0].get_color() == 'r', 'The line on the plot is not the correct color'\n",
         "failure_message": "The plot is not red.",
         "hidden": false,
         "locked": false,
         "points": 6,
         "success_message": "The plot is red as expected."
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6 - Exploring DataTypes": {
     "name": "q6 - Exploring DataTypes",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> import drexel_jupyter_logger\n>>> from ENGR131_Util_2024 import submit_score, get_string_from_encrypted_string\n>>> from unittest.mock import patch\n>>> import numpy as np\n>>> import json\n>>> from ENGR131_Util_2024 import responses, ResponseStore\n>>> responder = ResponseStore()\n>>> responder.create_file()\n>>> points_ = [3, 3, 3, 3, 6, 6, 6]\n>>> for (i, point) in enumerate(points_):\n...     drexel_jupyter_logger.variable_logger_csv(f'0, {point}', f'q6_{i + 1}')\n>>> if 'drexel_email' not in responses:\n...     raise ValueError('Please fill out the student info form and run the test again')\n>>> scorer = submit_score()\n>>> question_id = 'q6_1'\n>>> max_score = 3\n>>> score = 0\n>>> out = get_string_from_encrypted_string(b'gAAAAABkDQ71dNmKvaPlZOj3cLT2FVfh66b09i0yV2Fl08bcFY0K_Io7ju7utXkX3cCmsb9HMsHTAHERX0X48acRp1MGHYXxSg==')\n>>> if str(int_) == out and isinstance(int_, int):\n...     score = max_score\n>>> drexel_jupyter_logger.variable_logger_csv(f'{score}, {max_score}', question_id)\n>>> response = {'question_id': question_id, 'score': score, 'max_score': max_score, 'student_response': int_}\n>>> responder.add_response(response)\n>>> assert isinstance(int_, int)\n>>> assert str(int_) == out, 'int_ incorrectly implemented.'\n",
         "failure_message": "int_ incorrectly implemented.",
         "hidden": false,
         "locked": false,
         "points": 3,
         "success_message": "int_ correctly implemented."
        },
        {
         "code": ">>> import drexel_jupyter_logger\n>>> from ENGR131_Util_2024 import submit_score, get_string_from_encrypted_string\n>>> from unittest.mock import patch\n>>> import numpy as np\n>>> import json\n>>> from ENGR131_Util_2024 import responses, ResponseStore\n>>> responder = ResponseStore()\n>>> if 'drexel_email' not in responses:\n...     raise ValueError('Please fill out the student info form and run the test again')\n>>> scorer = submit_score()\n>>> question_id = 'q6_2'\n>>> max_score = 3\n>>> score = 0\n>>> out = get_string_from_encrypted_string(b'gAAAAABkDRABl3uUfW0G0yLb5kCpVpz5XBRYUjowgJ9_c9CNAGielysMhb5cp72zDF0Gi9abZrATkUoZfUGv7UJp65Cm7WvjiQ==')\n>>> if str(float_) == out and isinstance(float_, float):\n...     score = max_score\n>>> drexel_jupyter_logger.variable_logger_csv(f'{score}, {max_score}', question_id)\n>>> response = {'question_id': question_id, 'score': score, 'max_score': max_score, 'student_response': float_}\n>>> responder.add_response(response)\n>>> assert isinstance(float_, float)\n>>> assert str(float_) == out, 'float_ incorrectly implemented.'\n",
         "failure_message": "float_ incorrectly implemented.",
         "hidden": false,
         "locked": false,
         "points": 3,
         "success_message": "float_ correctly implemented."
        },
        {
         "code": ">>> import drexel_jupyter_logger\n>>> from ENGR131_Util_2024 import submit_score, get_string_from_encrypted_string\n>>> from unittest.mock import patch\n>>> import numpy as np\n>>> import json\n>>> from ENGR131_Util_2024 import responses, ResponseStore\n>>> responder = ResponseStore()\n>>> if 'drexel_email' not in responses:\n...     raise ValueError('Please fill out the student info form and run the test again')\n>>> scorer = submit_score()\n>>> question_id = 'q6_3'\n>>> max_score = 3\n>>> score = 0\n>>> out = get_string_from_encrypted_string(b'gAAAAABkDRDR42csyMOkhel0VkaN7JXse5_3OFTam1ryIyiJvEdIcfDCfocojIiGZU2PPJQm-rrfBNXLyzMQ-QJgmZi7qEdthA==')\n>>> if str(bool_) == out and isinstance(bool_, bool):\n...     score = max_score\n>>> drexel_jupyter_logger.variable_logger_csv(f'{score}, {max_score}', question_id)\n>>> response = {'question_id': question_id, 'score': score, 'max_score': max_score, 'student_response': bool_}\n>>> responder.add_response(response)\n>>> assert isinstance(bool_, bool)\n>>> assert str(bool_) == out, 'bool_ incorrectly implemented.'\n",
         "failure_message": "bool_ incorrectly implemented.",
         "hidden": false,
         "locked": false,
         "points": 3,
         "success_message": "bool_ correctly implemented."
        },
        {
         "code": ">>> import drexel_jupyter_logger\n>>> from ENGR131_Util_2024 import submit_score, get_string_from_encrypted_string\n>>> from unittest.mock import patch\n>>> import numpy as np\n>>> import json\n>>> from ENGR131_Util_2024 import responses, ResponseStore\n>>> responder = ResponseStore()\n>>> if 'drexel_email' not in responses:\n...     raise ValueError('Please fill out the student info form and run the test again')\n>>> scorer = submit_score()\n>>> question_id = 'q6_4'\n>>> max_score = 3\n>>> score = 0\n>>> out = get_string_from_encrypted_string(b'gAAAAABkDRECpI3kPdhirMzwLAddFoJKDXs7mXJ5AWPYJTx_gTKGYY9VR0REjaPKLCAc0MQ5UuSr4cMcBBc8zSVg2GT6LiDvLg==')\n>>> if str(string_) == out and isinstance(string_, str):\n...     score = max_score\n>>> drexel_jupyter_logger.variable_logger_csv(f'{score}, {max_score}', question_id)\n>>> response = {'question_id': question_id, 'score': score, 'max_score': max_score, 'student_response': str(string_)}\n>>> responder.add_response(response)\n>>> assert isinstance(string_, str)\n>>> assert str(string_) == out, 'string_ incorrectly implemented.'\n",
         "failure_message": "string_ incorrectly implemented.",
         "hidden": false,
         "locked": false,
         "points": 3,
         "success_message": "string_ correctly implemented."
        },
        {
         "code": ">>> import drexel_jupyter_logger\n>>> from ENGR131_Util_2024 import submit_score, get_string_from_encrypted_string\n>>> from unittest.mock import patch\n>>> import numpy as np\n>>> import json\n>>> from ENGR131_Util_2024 import responses, ResponseStore\n>>> responder = ResponseStore()\n>>> if 'drexel_email' not in responses:\n...     raise ValueError('Please fill out the student info form and run the test again')\n>>> scorer = submit_score()\n>>> question_id = 'q6_5'\n>>> max_score = 6\n>>> score = 0\n>>> out = get_string_from_encrypted_string(b'gAAAAABkDRFLoBpFP_06PNjYcIdyOpLGFYYboKIjMjTAu936eLjoKByeaF5XTyYcvGh5PWyVVU1kFUqWj4YeqbRDfMTRpPYXGw==')\n>>> if str(list_) == out and isinstance(list_, list):\n...     score = max_score\n>>> drexel_jupyter_logger.variable_logger_csv(f'{score}, {max_score}', question_id)\n>>> response = {'question_id': question_id, 'score': score, 'max_score': max_score, 'student_response': str(list_)}\n>>> responder.add_response(response)\n>>> assert isinstance(list_, list)\n>>> assert str(list_) == out, 'list_ incorrectly implemented.'\n",
         "failure_message": "list_ incorrectly implemented.",
         "hidden": false,
         "locked": false,
         "points": 6,
         "success_message": "list_ correctly implemented."
        },
        {
         "code": ">>> import drexel_jupyter_logger\n>>> from ENGR131_Util_2024 import submit_score, get_string_from_encrypted_string\n>>> from unittest.mock import patch\n>>> import numpy as np\n>>> import json\n>>> from ENGR131_Util_2024 import responses, ResponseStore\n>>> responder = ResponseStore()\n>>> if 'drexel_email' not in responses:\n...     raise ValueError('Please fill out the student info form and run the test again')\n>>> scorer = submit_score()\n>>> question_id = 'q6_6'\n>>> max_score = 6\n>>> score = 0\n>>> out = get_string_from_encrypted_string(b'gAAAAABkDRGEX2Cl-qHsEg1NFnwyZSHVLsjlc1JJWn4Lj76LT7dsEoNqlBpXQry0zKyFS2oCSzdJbYSm95SKxB96pM_HHsyuNw==')\n>>> if str(tuple_) == out and isinstance(tuple_, tuple):\n...     score = max_score\n>>> drexel_jupyter_logger.variable_logger_csv(f'{score}, {max_score}', question_id)\n>>> response = {'question_id': question_id, 'score': score, 'max_score': max_score, 'student_response': str(tuple_)}\n>>> responder.add_response(response)\n>>> assert isinstance(tuple_, tuple)\n>>> assert str(tuple_) == out, 'tuple_ incorrectly implemented.'\n",
         "failure_message": "tuple_ incorrectly implemented.",
         "hidden": false,
         "locked": false,
         "points": 6,
         "success_message": "tuple_ correctly implemented."
        },
        {
         "code": ">>> import drexel_jupyter_logger\n>>> from ENGR131_Util_2024 import submit_score, get_string_from_encrypted_string\n>>> from unittest.mock import patch\n>>> import numpy as np\n>>> import json\n>>> from ENGR131_Util_2024 import responses, ResponseStore\n>>> responder = ResponseStore()\n>>> if 'drexel_email' not in responses:\n...     raise ValueError('Please fill out the student info form and run the test again')\n>>> scorer = submit_score()\n>>> question_id = 'q6_7'\n>>> max_score = 6\n>>> score = 0\n>>> out = get_string_from_encrypted_string(b'gAAAAABkDRG1TzclExNe_t7qeJgaHYC9uWufJOfW5u7L198U2jtPxlFPQTnL9qj0MFHrNC7pIUfpz7od9FbuyQy6nuNnEFvlk2ZWGw3xRiU_wh4fVtRuV-Y=')\n>>> if str(dict_) == out and isinstance(dict_, dict):\n...     score = max_score\n>>> drexel_jupyter_logger.variable_logger_csv(f'{score}, {max_score}', question_id)\n>>> response = {'question_id': question_id, 'score': score, 'max_score': max_score, 'student_response': str(dict_)}\n>>> responder.add_response(response)\n>>> assert isinstance(dict_, dict)\n>>> assert str(dict_) == out, 'dict_ incorrectly implemented.'\n",
         "failure_message": "dict_ incorrectly implemented.",
         "hidden": false,
         "locked": false,
         "points": 6,
         "success_message": "dict_ correctly implemented."
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q7-Building a Thermometer": {
     "name": "q7-Building a Thermometer",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> import drexel_jupyter_logger\n>>> from ENGR131_Util_2024 import submit_score, get_string_from_encrypted_string\n>>> from unittest.mock import patch\n>>> import numpy as np\n>>> import json\n>>> from ENGR131_Util_2024 import responses, ResponseStore\n>>> responder = ResponseStore()\n>>> responder.create_file()\n>>> points_ = [6, 6, 8, 8, 8]\n>>> for (i, point) in enumerate(points_):\n...     drexel_jupyter_logger.variable_logger_csv(f'0, {point}', f'q7_{i + 1}')\n>>> if 'drexel_email' not in responses:\n...     raise ValueError('Please fill out the student info form and run the test again')\n>>> scorer = submit_score()\n>>> question_id = 'q7_1'\n>>> max_score = 6\n>>> score = 0\n>>> if isinstance(Thermometer, type):\n...     score = max_score\n>>> drexel_jupyter_logger.variable_logger_csv(f'{score}, {max_score}', question_id)\n>>> response = {'question_id': question_id, 'score': score, 'max_score': max_score, 'student_response': str(isinstance(Thermometer, type))}\n>>> responder.add_response(response)\n>>> assert isinstance(Thermometer, type), 'The Class Thermometer is not implemented.'\n",
         "failure_message": "The Class Thermometer is not implemented.",
         "hidden": false,
         "locked": false,
         "points": 6,
         "success_message": "The Class Thermometer is implemented."
        },
        {
         "code": ">>> import drexel_jupyter_logger\n>>> from ENGR131_Util_2024 import submit_score, get_string_from_encrypted_string\n>>> from unittest.mock import patch\n>>> import numpy as np\n>>> import json\n>>> from ENGR131_Util_2024 import responses, ResponseStore\n>>> responder = ResponseStore()\n>>> if 'drexel_email' not in responses:\n...     raise ValueError('Please fill out the student info form and run the test again')\n>>> scorer = submit_score()\n>>> question_id = 'q7_2'\n>>> max_score = 6\n>>> score = 0\n>>> t = Thermometer(25)\n>>> if hasattr(t, 'temperature_c') and t.temperature_c == 25:\n...     score = max_score\n>>> drexel_jupyter_logger.variable_logger_csv(f'{score}, {max_score}', question_id)\n>>> response = {'question_id': question_id, 'score': score, 'max_score': max_score, 'student_response': str(t.temperature_c)}\n>>> responder.add_response(response)\n>>> assert hasattr(t, 'temperature_c'), \"Thermometer does not have 'temperature' attribute\"\n>>> assert t.temperature_c == 25, 'Thermometer initialization did not set attribute temperature correctly'\n",
         "failure_message": "The Class Thermometer initialization is implemented incorrectly.",
         "hidden": false,
         "locked": false,
         "points": 6,
         "success_message": "The Class Thermometer initialization is implemented correctly."
        },
        {
         "code": ">>> import drexel_jupyter_logger\n>>> from ENGR131_Util_2024 import submit_score, get_string_from_encrypted_string\n>>> from unittest.mock import patch\n>>> import numpy as np\n>>> import json\n>>> from ENGR131_Util_2024 import responses, ResponseStore\n>>> responder = ResponseStore()\n>>> if 'drexel_email' not in responses:\n...     raise ValueError('Please fill out the student info form and run the test again')\n>>> scorer = submit_score()\n>>> question_id = 'q7_3'\n>>> max_score = 8\n>>> score = 0\n>>> t = Thermometer(25)\n>>> t.set_temperature(50)\n>>> if t.temperature_c == 50.0:\n...     score += 4\n>>> t_ = Thermometer(25)\n>>> t_.set_temperature('abc')\n>>> out = str(t_.temperature_c)\n>>> if out == 'fail':\n...     score += 4\n>>> drexel_jupyter_logger.variable_logger_csv(f'{score}, {max_score}', question_id)\n>>> response = {'question_id': question_id, 'score': score, 'max_score': max_score, 'student_response': str([t.temperature_c, t_.temperature_c])}\n>>> responder.add_response(response)\n>>> t = Thermometer(25)\n>>> t.set_temperature(50)\n>>> assert t.temperature_c == 50.0, 'set temperature does not function properly'\n>>> t = Thermometer(25)\n>>> t.set_temperature('abc')\n>>> out = str(t.temperature_c)\n>>> assert out == 'fail', 'set temperature does not return the correct response when a unsuitable input is provided'\n",
         "failure_message": "set_temperature can not correctly handle non numeric inputs.",
         "hidden": false,
         "locked": false,
         "points": 8,
         "success_message": "set_temperature can correctly handle non numeric inputs."
        },
        {
         "code": ">>> import drexel_jupyter_logger\n>>> from ENGR131_Util_2024 import submit_score, get_string_from_encrypted_string\n>>> from unittest.mock import patch\n>>> import numpy as np\n>>> import json\n>>> from ENGR131_Util_2024 import responses, ResponseStore\n>>> responder = ResponseStore()\n>>> if 'drexel_email' not in responses:\n...     raise ValueError('Please fill out the student info form and run the test again')\n>>> scorer = submit_score()\n>>> question_id = 'q7_4'\n>>> max_score = 8\n>>> score = 0\n>>> t = Thermometer(0)\n>>> if t.to_fahrenheit() == 32.0:\n...     score += 4\n>>> hold = t.to_fahrenheit()\n>>> t.set_temperature(20)\n>>> if t.to_fahrenheit() == 68.0:\n...     score += 4\n>>> drexel_jupyter_logger.variable_logger_csv(f'{score}, {max_score}', question_id)\n>>> response = {'question_id': question_id, 'score': score, 'max_score': max_score, 'student_response': str([hold, t.to_fahrenheit()])}\n>>> responder.add_response(response)\n>>> t = Thermometer(0)\n>>> assert t.to_fahrenheit() == 32.0, 'Incorrect conversion to Fahrenheit.'\n>>> t.set_temperature(20)\n>>> assert t.to_fahrenheit() == 68.0, 'Incorrect conversion to Fahrenheit.'\n",
         "failure_message": "Incorrect conversion to Fahrenheit.",
         "hidden": false,
         "locked": false,
         "points": 8,
         "success_message": "Correct conversion to Fahrenheit."
        },
        {
         "code": ">>> import drexel_jupyter_logger\n>>> from ENGR131_Util_2024 import submit_score, get_string_from_encrypted_string\n>>> from unittest.mock import patch\n>>> import numpy as np\n>>> import json\n>>> from ENGR131_Util_2024 import responses, ResponseStore\n>>> responder = ResponseStore()\n>>> if 'drexel_email' not in responses:\n...     raise ValueError('Please fill out the student info form and run the test again')\n>>> scorer = submit_score()\n>>> question_id = 'q7_5'\n>>> max_score = 8\n>>> score = 0\n>>> conditions = [hasattr(water_sensor, 'temperature_c'), hasattr(water_sensor, 'to_fahrenheit'), temp_c == 12, temp_f == 53.6]\n>>> statements = [\"Thermometer does not have 'temperature' attribute\", \"Thermometer does not have 'to_fahrenheit' attribute\", 'Incorrect temperature for temp_c.', 'Incorrect temperature for temp_f.']\n>>> for condition in conditions:\n...     if condition:\n...         score += 2\n>>> drexel_jupyter_logger.variable_logger_csv(f'{score}, {max_score}', question_id)\n>>> response = {'question_id': question_id, 'score': score, 'max_score': max_score, 'student_response': str(conditions)}\n>>> responder.add_response(response)\n>>> for (condition, statement) in zip(conditions, statements):\n...     assert condition, statement\n",
         "failure_message": "Incorrect conversion to Fahrenheit.",
         "hidden": false,
         "locked": false,
         "points": 8,
         "success_message": "Correct conversion to Fahrenheit."
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q8-Determining a Price for a Circuit": {
     "name": "q8-Determining a Price for a Circuit",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> import drexel_jupyter_logger\n>>> from ENGR131_Util_2024 import submit_score, get_string_from_encrypted_string\n>>> from unittest.mock import patch\n>>> import numpy as np\n>>> import json\n>>> from ENGR131_Util_2024 import responses, ResponseStore\n>>> responder = ResponseStore()\n>>> responder.create_file()\n>>> points_ = [0, 1, 0, 6, 15, 12, 6]\n>>> for (i, point) in enumerate(points_):\n...     drexel_jupyter_logger.variable_logger_csv(f'0, {point}', f'q8_{i + 1}')\n>>> if 'drexel_email' not in responses:\n...     raise ValueError('Please fill out the student info form and run the test again')\n>>> scorer = submit_score()\n>>> question_id = 'q8_1'\n>>> max_score = 0\n>>> score = 0\n>>> conditions = [parts_cost['10-ohm resistor'] == 0.025, parts_cost['100-ohm resistor'] == 0.03, parts_cost['1-pF capacitor'] == 0.05, parts_cost['10-pF capacitor'] == 0.12]\n>>> statements = ['10-ohm resistor not correctly defined', '100-ohm resistor not correctly defined', '1-pF capacitor not correctly defined', '10-pF capacitor not correctly defined']\n>>> for condition in conditions:\n...     if condition:\n...         score += 0\n>>> drexel_jupyter_logger.variable_logger_csv(f'{score}, {max_score}', question_id)\n>>> response = {'question_id': question_id, 'score': score, 'max_score': max_score, 'student_response': str(conditions)}\n>>> responder.add_response(response)\n>>> for (condition, statement) in zip(conditions, statements):\n...     assert condition, statement\n",
         "failure_message": "parts_cost is incorrect. Please make sure you did not modify the dictionary.",
         "hidden": false,
         "locked": false,
         "points": 0,
         "success_message": "parts_cost is correct."
        },
        {
         "code": ">>> import drexel_jupyter_logger\n>>> from ENGR131_Util_2024 import submit_score, get_string_from_encrypted_string\n>>> from unittest.mock import patch\n>>> import json\n>>> from ENGR131_Util_2024 import responses, ResponseStore\n>>> responder = ResponseStore()\n>>> if 'drexel_email' not in responses:\n...     raise ValueError('Please fill out the student info form and run the test again')\n>>> scorer = submit_score()\n>>> question_id = 'q8_2'\n>>> max_score = 1\n>>> score = 0\n>>> if np.__version__ is not None:\n...     score = max_score\n>>> drexel_jupyter_logger.variable_logger_csv(f'{score}, {max_score}', question_id)\n>>> response = {'question_id': question_id, 'score': score, 'max_score': max_score, 'student_response': str(np.__version__)}\n>>> responder.add_response(response)\n>>> assert np.__version__ is not None, 'numpy is incorrectly imported'\n",
         "failure_message": "numpy incorrectly implemented.",
         "hidden": false,
         "locked": false,
         "points": 1,
         "success_message": "numpy correctly implemented."
        },
        {
         "code": ">>> import drexel_jupyter_logger\n>>> from ENGR131_Util_2024 import submit_score, get_string_from_encrypted_string\n>>> from unittest.mock import patch\n>>> import json\n>>> from ENGR131_Util_2024 import responses, ResponseStore\n>>> responder = ResponseStore()\n>>> if 'drexel_email' not in responses:\n...     raise ValueError('Please fill out the student info form and run the test again')\n>>> scorer = submit_score()\n>>> question_id = 'q8_3'\n>>> max_score = 0\n>>> score = 0\n>>> conditions = [circuit_parts['10-ohm resistor']['quantity'] == 10, circuit_parts['10-pF capacitor']['quantity'] == 3, circuit_parts['1-pF capacitor']['quantity'] == 7, circuit_parts['Custom Processor']['quantity'] == 2, circuit_parts['Custom Processor']['price'] == 12]\n>>> statements = ['10-ohm resistor quantity not correctly defined', '10-pF capacitor quantity not correctly defined', '1-pF capacitor quantity not correctly defined', 'Custom Processor quantity not correctly defined', 'Custom Processor price not correctly defined']\n>>> for condition in conditions:\n...     if condition:\n...         score += 0\n>>> drexel_jupyter_logger.variable_logger_csv(f'{score}, {max_score}', question_id)\n>>> response = {'question_id': question_id, 'score': score, 'max_score': max_score, 'student_response': str(conditions)}\n>>> responder.add_response(response)\n>>> for (condition, statement) in zip(conditions, statements):\n...     assert condition, statement\n",
         "failure_message": "circuit_parts is incorrect. Please make sure you did not modify the dictionary.",
         "hidden": false,
         "locked": false,
         "points": 0,
         "success_message": "circuit_parts is correct."
        },
        {
         "code": ">>> import drexel_jupyter_logger\n>>> from ENGR131_Util_2024 import submit_score, get_string_from_encrypted_string\n>>> from unittest.mock import patch\n>>> import json\n>>> from ENGR131_Util_2024 import responses, ResponseStore\n>>> responder = ResponseStore()\n>>> if 'drexel_email' not in responses:\n...     raise ValueError('Please fill out the student info form and run the test again')\n>>> scorer = submit_score()\n>>> question_id = 'q8_4'\n>>> max_score = 6\n>>> score = 0\n>>> conditions = [price_calculator.__name__ == 'price_calculator', price_calculator.__code__.co_argcount == 3]\n>>> statements = ['you did not name price_calculator correctly', 'price calculator does not have the correct number of inputs']\n>>> for condition in conditions:\n...     if condition:\n...         score += 3\n>>> drexel_jupyter_logger.variable_logger_csv(f'{score}, {max_score}', question_id)\n>>> response = {'question_id': question_id, 'score': score, 'max_score': max_score, 'student_response': str([price_calculator.__name__, price_calculator.__code__.co_argcount])}\n>>> responder.add_response(response)\n>>> for (condition, statement) in zip(conditions, statements):\n...     assert condition, statement\n",
         "failure_message": "price_calculator call is incorrect.",
         "hidden": false,
         "locked": false,
         "points": 6,
         "success_message": "price_calculator call is correct."
        },
        {
         "code": ">>> import drexel_jupyter_logger\n>>> from ENGR131_Util_2024 import submit_score, get_string_from_encrypted_string\n>>> from unittest.mock import patch\n>>> import json\n>>> from ENGR131_Util_2024 import responses, ResponseStore\n>>> responder = ResponseStore()\n>>> if 'drexel_email' not in responses:\n...     raise ValueError('Please fill out the student info form and run the test again')\n>>> scorer = submit_score()\n>>> question_id = 'q8_5'\n>>> max_score = 15\n>>> score = 0\n>>> with patch('builtins.print') as mock_print:\n...     out = get_string_from_encrypted_string(b'gAAAAABkDz6Y_aio_akqMGkihfI5zKR2KkUg0yXVVHNrVV82qunlAwRjVZN2R2OYzUqkBBLBG-qd4t_2Vpx-iHxYPLMGTYQsFA==')\n...     if str(price_calculator(parts_cost, circuit_parts, 0.2)) == out:\n...         score = max_score\n>>> drexel_jupyter_logger.variable_logger_csv(f'{score}, {max_score}', question_id)\n>>> response = {'question_id': question_id, 'score': score, 'max_score': max_score, 'student_response': str(out)}\n>>> responder.add_response(response)\n>>> with patch('builtins.print') as mock_print:\n...     assert str(price_calculator(parts_cost, circuit_parts, 0.2)) == out, 'price_calculator  computation is incorrect.'\n",
         "failure_message": "price_calculator computation is incorrect.",
         "hidden": false,
         "locked": false,
         "points": 15,
         "success_message": "price_calculator computation is correct."
        },
        {
         "code": ">>> import drexel_jupyter_logger\n>>> from ENGR131_Util_2024 import submit_score, get_string_from_encrypted_string\n>>> from unittest.mock import patch\n>>> import json\n>>> from ENGR131_Util_2024 import responses, ResponseStore\n>>> responder = ResponseStore()\n>>> if 'drexel_email' not in responses:\n...     raise ValueError('Please fill out the student info form and run the test again')\n>>> scorer = submit_score()\n>>> question_id = 'q8_6'\n>>> max_score = 12\n>>> score = 0\n>>> parts_2 = parts_cost.copy()\n>>> parts_2['10-ohm resistor'] = 10\n>>> with patch('builtins.print') as mock_print:\n...     out = get_string_from_encrypted_string(b'gAAAAABkD3dtfMyEiX9AYR1pum4OZ5xSbbZ367hae3wxg5SSAcor4m5CXO6u1OthJkNgyGQUotRmCY2v18f4rjxK7Z3kbCNC3A==')\n...     if out == str(price_calculator(parts_2, circuit_parts, 0.2)):\n...         score = max_score\n>>> drexel_jupyter_logger.variable_logger_csv(f'{score}, {max_score}', question_id)\n>>> response = {'question_id': question_id, 'score': score, 'max_score': max_score, 'student_response': str(out)}\n>>> responder.add_response(response)\n>>> with patch('builtins.print') as mock_print:\n...     assert out == str(price_calculator(parts_2, circuit_parts, 0.2)), 'Your code does not use the parts_cost correctly'\n",
         "failure_message": "Your code does not use the parts_cost correctly.",
         "hidden": false,
         "locked": false,
         "points": 12,
         "success_message": "Your code correctly uses the parts_cost."
        },
        {
         "code": ">>> import drexel_jupyter_logger\n>>> from ENGR131_Util_2024 import submit_score, get_string_from_encrypted_string\n>>> from unittest.mock import patch\n>>> import json\n>>> from ENGR131_Util_2024 import responses, ResponseStore\n>>> responder = ResponseStore()\n>>> if 'drexel_email' not in responses:\n...     raise ValueError('Please fill out the student info form and run the test again')\n>>> scorer = submit_score()\n>>> question_id = 'q8_7'\n>>> max_score = 6\n>>> score = 0\n>>> with patch('builtins.print') as mock_print:\n...     out = get_string_from_encrypted_string(b'gAAAAABkD3D3Z-O5XJGgN67FRSetS5oiOetUTzWxQFbw_OTKnrjBEhnelTKcH0d22z4AneFL8pYrtOgSg0Q2qYc0A4dI-VdzzcV6pXrv5aKw-FAVmhHMZ7U=')\n...     if print_total(price_calculator(parts_cost, circuit_parts, profit)) == out:\n...         score = max_score\n>>> drexel_jupyter_logger.variable_logger_csv(f'{score}, {max_score}', question_id)\n>>> response = {'question_id': question_id, 'score': score, 'max_score': max_score, 'student_response': str(out)}\n>>> responder.add_response(response)\n>>> with patch('builtins.print') as mock_print:\n...     assert print_total(price_calculator(parts_cost, circuit_parts, profit)) == out, 'The string being printed is not correct'\n",
         "failure_message": "print message is incorrect.",
         "hidden": false,
         "locked": false,
         "points": 6,
         "success_message": "print message is correct."
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q9-Plotting and Fitting": {
     "name": "q9-Plotting and Fitting",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> import drexel_jupyter_logger\n>>> from ENGR131_Util_2024 import submit_score, get_string_from_encrypted_string\n>>> from unittest.mock import patch\n>>> import json\n>>> from ENGR131_Util_2024 import responses, ResponseStore\n>>> responder = ResponseStore()\n>>> responder.create_file()\n>>> points_ = [3, 9, 4, 6, 3, 9, 3, 4, 4, 3, 3, 6, 6]\n>>> for (i, point) in enumerate(points_):\n...     drexel_jupyter_logger.variable_logger_csv(f'0, {point}', f'q9_{i + 1}')\n>>> if 'drexel_email' not in responses:\n...     raise ValueError('Please fill out the student info form and run the test again')\n>>> scorer = submit_score()\n>>> question_id = 'q9_1'\n>>> max_score = 3\n>>> score = 0\n>>> conditions = [np.__name__ == 'numpy', plt.__name__ == 'matplotlib.pyplot', 'curve_fit' in dir()]\n>>> statements = ['numpy not imported correctly', 'matplotlib.pyplot not imported correctly', 'scipy.optimize.curve_fit not imported correctly']\n>>> for condition in conditions:\n...     if condition:\n...         score += 1\n>>> drexel_jupyter_logger.variable_logger_csv(f'{score}, {max_score}', question_id)\n>>> response = {'question_id': question_id, 'score': score, 'max_score': max_score, 'student_response': str(plt.__name__)}\n>>> responder.add_response(response)\n>>> for (condition, statement) in zip(conditions, statements):\n...     assert condition, statement\n",
         "failure_message": "Your input statements are incorrect.",
         "hidden": false,
         "locked": false,
         "points": 3,
         "success_message": "Your input statements are correct."
        },
        {
         "code": ">>> import drexel_jupyter_logger\n>>> from ENGR131_Util_2024 import submit_score, get_string_from_encrypted_string\n>>> from unittest.mock import patch\n>>> import json\n>>> from ENGR131_Util_2024 import responses, ResponseStore\n>>> responder = ResponseStore()\n>>> if 'drexel_email' not in responses:\n...     raise ValueError('Please fill out the student info form and run the test again')\n>>> scorer = submit_score()\n>>> question_id = 'q9_2'\n>>> max_score = 9\n>>> score = 0\n>>> x_test = np.array([0, 1, 2])\n>>> (a_test, b_test, c_test) = (1, 1, 1)\n>>> expected_output = np.array([0.60653066, 1.0, 0.60653066])\n>>> conditions = [np.allclose(gaussian(x_test, a_test, b_test, c_test), expected_output)]\n>>> statements = ['gaussian function is not implemented correctly']\n>>> for condition in conditions:\n...     if condition:\n...         score += 9\n>>> drexel_jupyter_logger.variable_logger_csv(f'{score}, {max_score}', question_id)\n>>> response = {'question_id': question_id, 'score': score, 'max_score': max_score, 'student_response': str([*gaussian(x_test, a_test, b_test, c_test)])}\n>>> responder.add_response(response)\n>>> for (condition, statement) in zip(conditions, statements):\n...     assert condition, statement\n",
         "failure_message": "gaussian function is not implemented correctly",
         "hidden": false,
         "locked": false,
         "points": 9,
         "success_message": "gaussian function is implemented correctly"
        },
        {
         "code": ">>> import drexel_jupyter_logger\n>>> from ENGR131_Util_2024 import submit_score, get_string_from_encrypted_string\n>>> from unittest.mock import patch\n>>> import json\n>>> from ENGR131_Util_2024 import responses, ResponseStore\n>>> responder = ResponseStore()\n>>> if 'drexel_email' not in responses:\n...     raise ValueError('Please fill out the student info form and run the test again')\n>>> scorer = submit_score()\n>>> question_id = 'q9_3'\n>>> max_score = 4\n>>> score = 0\n>>> conditions = [np.allclose(x_data[0], -20), np.allclose(x_data[-1], 20), len(x_data) == 100, np.isclose(sum(x_data[3::3]), 19.99999999999997)]\n>>> statements = ['x_data is not defined correctly', 'x_data is not defined correctly', 'x_data length is not correct', 'x_data is not defined correctly']\n>>> for condition in conditions:\n...     if condition:\n...         score += 1\n>>> drexel_jupyter_logger.variable_logger_csv(f'{score}, {max_score}', question_id)\n>>> response = {'question_id': question_id, 'score': score, 'max_score': max_score, 'student_response': str(conditions)}\n>>> responder.add_response(response)\n>>> for (condition, statement) in zip(conditions, statements):\n...     assert condition, statement\n",
         "failure_message": "x_data is not defined correctly.",
         "hidden": false,
         "locked": false,
         "points": 4,
         "success_message": "x_data is defined correctly."
        },
        {
         "code": ">>> import drexel_jupyter_logger\n>>> from ENGR131_Util_2024 import submit_score, get_string_from_encrypted_string\n>>> from unittest.mock import patch\n>>> import json\n>>> from ENGR131_Util_2024 import responses, ResponseStore\n>>> responder = ResponseStore()\n>>> if 'drexel_email' not in responses:\n...     raise ValueError('Please fill out the student info form and run the test again')\n>>> scorer = submit_score()\n>>> question_id = 'q9_4'\n>>> max_score = 6\n>>> score = 0\n>>> with patch('builtins.print') as mock_print:\n...     out = get_string_from_encrypted_string(b'gAAAAABkD-NzYrMglFaqNyS-zlkoQo-gwxc1YCD39uP-uNVNborNwKBB7YKbPEpayy9rq7hUGxJhZWrWmXBDQ14fxazkpMi5v-8ArClYcJqu2ZJvUs2LKzPwphZZ63pfZBxs8IK0DIw-EnU5mRoiaMT3YoTAD7rSGUrnDdqM44YGV3b_Gyt7mYE0n_obgjbXngI0DbuJzExiXeA4KQNpSsKJD6x870owlz7wNXADMhSrU03suFLlUx1pgGwJsg-7lQhUocs64ZahOJvSfABgqu_Ml_SDkaudvTyHTFCPy-bnq3xn-Cc4d9jmsdOxUmqMvl3xLZw1ALt6YrMBdlNZh0hBgjrT4iVP9tySJBG8dnov7TLz9_PuWRGRulKG-tYZU9XJ8uuFC09mXpDLE49gdlCDp_jYbvYfWX6rPlgBd7pXyQ2YWAKajlRzuJ1Q7TrXSVm_Xbj2eF48_iehGgz2cuzg0YARRKXvrg3JFdRn-jRhN9Pop4SoJVEcIbh6p6AmU0u2rUTFDN5UazGdLSWav8g2xQyHCm8_2i9il6lVF3_H4E5gdycNi6eb4hUV0i7XY1rUzfulJYmJzWzQk3KgbR54In9AfLWvgHY0IcBduStXBmpMfwIG7NMbbSPhElQ0D6gXjA3F7zpkahXZ3S0ihF0bfHvY32pIfRVORtMT811bYif8OPTJH2i3YcHiaS7ElIUJXkb4dnJTOOVCIB08EV0ONZgZHwtpsFCCwCFvRTouorcfz1SXs9a0vtjtau3CgBI3cl0yAGGuiazfipZAEsbB68QedFffR_QRl0ZBN_IfGIMC8ksxUw3R9J46tUET_doctLBvd7LFbDTfkECNoKCh-E5x-POLyjeWgscHe64WSRNkYgKaKTWP70Y9OdYg2CpCZseE55ovOqgNjTVCMlRhuQWO9zuBSLAU3NOfL-ms4n9AWrx75Wsg9gxkIGVm36HZ-yKO_wJDcrRRQqtYaS0noeqEdo3-Gmj4ldMgPOATkNXKLoFJ7P6FfnNYUiY7mPykn1_1tqlBEhVvSXrAteRNr6E_8s6lgFr4l2kDU8s8_XSgKu_wazGqw45WVMiO5Zgsh2SXh4VnnUE0LFubdThZsKCplGr_uJK2IFc0jiWlSid8rE2GiLU69Y0mVkwod2PQOfIfJRzeNkAA9raRbs5CkFO8XFVgLGjKWGIJw_8zKncKHZ-LgK0PVFJ2zMBMuN8fXofonS3v7Zm85gj16MGtfwIOVhvODG4BrOkZvI43TCM5pPRq1-5nqyJfJ6gpC0SWWwMrEF7YSZ64YIA5YmVlaquaB5Fam4p5O3EN8m83aQ4AcCBcRZAaimCtPIrIhK7SS394wkfQLFb8q_WUJe5RBHm3PoVsbjac3Vlu7rQTVByxSf9KdFNdtKTXJsMzLL0743ZMBmm4RA5UlQWE15AFeIyiSXP8B0TgFKe9J5g8W-eT8KWyqRw-x00_7lF0e-4tcB1B6CLOuk4kpkZk0WkucN0S1IzZRZsjXcOwmsoNDqFdpx7NrO08ZgmXqkSqwLCo99q0C6gpk2oX3-BBw9cDOgvFIpYE4A==')\n...     if str(y_true) == out:\n...         score = max_score\n>>> drexel_jupyter_logger.variable_logger_csv(f'{score}, {max_score}', question_id)\n>>> response = {'question_id': question_id, 'score': score, 'max_score': max_score, 'student_response': str(y_true)}\n>>> responder.add_response(response)\n>>> assert str(y_true) == out, 'y_true is incorrect'\n",
         "failure_message": "y_true is incorrect.",
         "hidden": false,
         "locked": false,
         "points": 6,
         "success_message": "y_true is correct."
        },
        {
         "code": ">>> import drexel_jupyter_logger\n>>> from ENGR131_Util_2024 import submit_score, get_string_from_encrypted_string\n>>> from unittest.mock import patch\n>>> import json\n>>> from ENGR131_Util_2024 import responses, ResponseStore\n>>> responder = ResponseStore()\n>>> if 'drexel_email' not in responses:\n...     raise ValueError('Please fill out the student info form and run the test again')\n>>> scorer = submit_score()\n>>> question_id = 'q9_5'\n>>> max_score = 3\n>>> score = 0\n>>> with patch('builtins.print') as mock_print:\n...     out = get_string_from_encrypted_string(b'gAAAAABkD-UVvxc6wECqC_U9taa6Art0EtaZg2i_WB0sp4FHCf3Ggyed8uUrljAkCPxg-AwYxN2jnqaPVgMh57F1OX1ftyjzR1SfhzBk5UAkxPaLIFHteLZY_7oK67GesWvX-jys-fhfKLktB_4ba54nXaVQP3K2xb60izc-42Y4VvddXu9Iggo5dy7VcJP8-6Ix_un8KqJo2J4CKYXFni2El-O9zrvvc1EcNYgvGdpzxFFeUHEHAP0AEqTp-Ow5D7T0x2iGcEPXtDUcmbgMG_t-HAQRr8ky9DNyuKx9k13pWT85O3jpLAtExhBYk2yXtzS9iU4pQEYGpYhdB2HQosUQcV5rXdgsC6pGcWlWhTab2uG32_Lt8-Y0jlb4VgALBAtxJGXuevGpJ7Pt1fzt7pjHikCVHcMbmkhLaWGzZq1uf6d6mbA-lxo1wGif7-ryFB4TTyze2HoxcGGgLI7vtuHgduBSd397OTTuS9PMMnR7oIXBZprj_NJrSmI79N4mMDlkWz-baZikhLw6LbThxinvd4HX6aQPVm3bPlV9M5Yc3lSqr2YQimUKqONnZGKxzcQx4TxfeRjdv8mS49Qey1KRU5zUlBS11NX-ScrqIFDAwFBsq6OpMSqd80rZquYE5E-IsHr0g4McffuDal20JZqXtP_kYMaT3Iv_KCa__9TKmpMHKJpQ44qhvzuBkqA59npHU05s5gbr5OpPwug1W_H7WTgjRYD-IaDU9cxxKWs9erfEkcCMh7peMvnBVwu2IY_W1u7ocNxXiMR2BJX6FbGO2P12t7D8vz3lyCJFHhkYggO06v9UF0L3-eX186gXNdegYnML3kVevZbOkRHFxvU2hoZ6dzLW8Z8SgaO9H1-Pac5Ima1Kh36EF1GH-93QGnSf6w2feXG9yByfSSbRharfHT6-79nYgl3EgGL0wo8imUaN8uwRhkxma8myGCAsNbkPKPiltTXfyv8yKEjc-qC5UivinYtFYYkhiDqIQvQDB4YURk0p4MxesM1rOgq6aML_jc7haRUcUes4ynGERvVt_Nc3bV03--BIzTZx4D2q3OMEUt25CyTU9oUr6r4C_JsrxhtK2d5U9qrLY6b-W-F-lBI3ZrRh_HcwHR9Hx7YTA7vRcXaOyYB-fUWjwXa2Gbxj1s9iEpzWf8lEisQk40FUhc2jSmtiZEn6MO3dO_YxJ3s9TE1Tmq9aK7gmb3zMbZn2gQyuHxzuvnUeJBi-DelNLvQXP7C3GYyMjA6TuajEzNY6VSVE7sKgQyw_vjE0QteWtkUpODlAbpT42M6laN3ahNeT7l2WE0D-wvjEHzflGOF7udat6QccodlHUge4DAJSS4vaTWs5J-aoSCmPNrYN65W3CIokMhxXS0VZ0n26ObdU384YVKdOTtUn796D7Mdk0ll0ImLLs_1RcihuZPkmEhE2rnWUINRBOLAyoQz6RrNGe-tC-uA3XNsxYbDQ9ReBPlU_R_BbkkgEzKtMV7juGtddtKDF-1lzhoahFX33CNMdWcuq189G87JsEED_tdmSloByFx3sEExB0uuj0--pnbH-CV1674xJ8x2WarCevNkOts1Aa0tDyE9v9v8NmX6cIKHsT8rkIhStCE1_Xknn2uMGlqNPp_5k30P5_mKeth6tti6uH-MMNO_qQ8V82Ki7X5u3HOmPgAa3_uJGD-JVvREQ8uNw8SVFpCmO-X207mkqYEFXO_Q=')\n...     if str(y_noisy) == out:\n...         score = max_score\n>>> drexel_jupyter_logger.variable_logger_csv(f'{score}, {max_score}', question_id)\n>>> response = {'question_id': question_id, 'score': score, 'max_score': max_score, 'student_response': str(len(y_noisy))}\n>>> responder.add_response(response)\n>>> assert str(y_noisy) == out, 'y_noisy is incorrect'\n",
         "failure_message": "y_noisy is not implmemented correctly",
         "hidden": false,
         "locked": false,
         "points": 3,
         "success_message": "y_noisy is implmemented correctly"
        },
        {
         "code": ">>> import drexel_jupyter_logger\n>>> from ENGR131_Util_2024 import submit_score, get_string_from_encrypted_string\n>>> from unittest.mock import patch\n>>> import json\n>>> from ENGR131_Util_2024 import responses, ResponseStore\n>>> responder = ResponseStore()\n>>> if 'drexel_email' not in responses:\n...     raise ValueError('Please fill out the student info form and run the test again')\n>>> scorer = submit_score()\n>>> question_id = 'q9_6'\n>>> max_score = 9\n>>> score = 0\n>>> with patch('builtins.print') as mock_print:\n...     out = get_string_from_encrypted_string(b'gAAAAABkD-VjEnvkOx1q5hIc6OiMyPE7iayGWAtZR_Kry0mvrQbCr0ffV9Lrck5j673YFnlWxRjo9qR44gKpYzUOzGhmCBpXKZ2Dec1hHWXhY4szHZZuirxAWmfMc66dj5dpA1bzjYFG')\n...     chars_to_remove = ['[', ']', '\\n']\n...     for char in chars_to_remove:\n...         out = out.replace(char, '')\n...     out = [float(x) for x in out.split(' ')]\n...     if np.allclose(popt, out):\n...         score = max_score\n>>> drexel_jupyter_logger.variable_logger_csv(f'{score}, {max_score}', question_id)\n>>> response = {'question_id': question_id, 'score': score, 'max_score': max_score, 'student_response': str([*popt])}\n>>> responder.add_response(response)\n>>> assert np.allclose(popt, out), 'popt is not returning the correct values'\n",
         "failure_message": "popt is not returning the correct values",
         "hidden": false,
         "locked": false,
         "points": 9,
         "success_message": "popt is returning the correct values"
        },
        {
         "code": ">>> import drexel_jupyter_logger\n>>> from ENGR131_Util_2024 import submit_score, get_string_from_encrypted_string\n>>> from unittest.mock import patch\n>>> import json\n>>> from ENGR131_Util_2024 import responses, ResponseStore\n>>> responder = ResponseStore()\n>>> if 'drexel_email' not in responses:\n...     raise ValueError('Please fill out the student info form and run the test again')\n>>> scorer = submit_score()\n>>> question_id = 'q9_7'\n>>> max_score = 3\n>>> score = 0\n>>> with patch('builtins.print') as mock_print:\n...     out = get_string_from_encrypted_string(b'gAAAAABkD-WzDT72FrJF_4PrNsTIhnppKucv8pAjRnotXNrslY7YlSdDyqgiXabo32_bwcFXYkokXar7Vl-Bl1cN4lD70-PDCbfDPsxVDcGs25T4Z8IV-i6P7SAwaZ-k00NmVsJ4sF62ClW9ssomSd0jgYtbgy4J5sF9_KO-6s0ke6_Pfi1mrx8c90J5ZYsAwB9NKm45dZ-IWfjH955EkoxoPQcdqBQ16RG1J0k5DqGaP6hpRpR4mRANk50AFcgkX9PLqnKr3Z3AH1s_ugMEvza1GRlj1aVmaeF0JcHsVw1MxpZLLhPK0ikm8TcyX7cy6O8wi3xvjk5By8SFT96a0pqK5ObXGIuFAN6XlBH56yic8ttcRBODzaRiH_97wGvi8eUP0NNGOlMZFb_mPc7E9URWZCnJ0Krr57iOj5ALWDPqtMysnRGg4VCI9l6ZAFv0dP_HqbZT7U7TK_dFVAWNWv5MPeyg8rcGInJ2ygH1o1G8jRJRSbEcHQj3-ZU2K7DCFqc6xg2EO_31Dz6VLWnEBLVW2OUIth16vPCcjbmJgGuW0mgRulbogMNLFEXpGKJwi8LimBL_gZ0pee4vd8nZDKaRR8pSoLXsHyTHf-O8vXHzji4RiUF7VSReSrbpai1xoC7uyztjO1Vjce1bMf1pP7e3JNIzpJ7sfAlP0PPiERdG58Xe_2Qx_oWFMtQeDt-ViS4AWK76KlWgsiX0cFqWs3Dj_KMs1hmtywbEZtLkUjEUH6rclgw482Tp44fFQcUqBOOD92DJkGjmEr-FcVJtlu8R71hafSMBnOJ_cEUvZL-P5ljNUoUoM1I7REUK9hs-hiSYEyXbyCfv5dRSoXHOdNv0nDKoEcVWPYZPP3E74Qafmm47NHUNbcG3pMGXXMsOea7l5SnL_gfnYjNcvUIDEzZDXen2moiSP0P5EIeFTSItTdjSnM-TAMmDZlFSF3XtXD8vagpPb_-QdfPNOkkx1MQNjVXcKqDVPUwyuDkrqm0QFp2XrRW8Lwb7v5mENaD3HfZMlnpuBYAv9sXDtkb7Km30U9NgYExvScKEnBfbd61peROkRYXJiTA8uvx3xgWAeu47lCYNJ0jKomOQlJOVXH86TnPEjMKDQ9UUcFTJGcuQLaNfSPdSPqkf1gabg3e6VW4b--PSkhpss9OboUp1UOEo0KFnlj4Tl-zu8rwuWEUwlBUDYIyuKwWBYD_DpC62kBfCcuMuaM1xvbbMoedQLTm0OA0PIR_dP5nrW4IkGtnpbz43rqBMFCcGZ2un8KbCRT1mh8dB5nNdrtxl3dtBEB_nsl-M8gIqCruIfNHkoPx77SkXHT3zNnI66oYqgBpMCrTjShWB-xNtlRrKHqL0q9RpgPF9SJphcLBQ07eqV4s7TrJtfrhw41WuzLaCR7KYb_vsMj78KNrZmuaTDgGGu4lediwCYGOFA3I-jIxSj2jwYfDcrGKW9QUbDK5RTZ0_rEt6SwA5r7NpadXtv-RsGBycn-_vTNPQ_5PGIYGyY3GkQgVjiq0nBX1xI39PqmMKc0eyGKAH6pu2uhCw_FAe3GSSHzMke-zzIA==')\n...     chars_to_remove = ['[', ']', ' \\n']\n...     for char in chars_to_remove:\n...         out = out.replace(char, '')\n...     out = out.replace('  ', ' ')\n...     out = [float(x) for x in out.split(' ')]\n...     if np.allclose(y_fit, out):\n...         score = max_score\n>>> drexel_jupyter_logger.variable_logger_csv(f'{score}, {max_score}', question_id)\n>>> response = {'question_id': question_id, 'score': score, 'max_score': max_score, 'student_response': str([*popt])}\n>>> responder.add_response(response)\n>>> assert np.allclose(y_fit, out), 'y_fit is returning incorrect values'\n",
         "failure_message": "y_fit is returning incorrect values",
         "hidden": false,
         "locked": false,
         "points": 3,
         "success_message": "y_fit is returning correct values"
        },
        {
         "code": ">>> import drexel_jupyter_logger\n>>> from ENGR131_Util_2024 import submit_score, get_string_from_encrypted_string\n>>> from unittest.mock import patch\n>>> import json\n>>> from ENGR131_Util_2024 import responses, ResponseStore\n>>> responder = ResponseStore()\n>>> if 'drexel_email' not in responses:\n...     raise ValueError('Please fill out the student info form and run the test again')\n>>> scorer = submit_score()\n>>> question_id = 'q9_8'\n>>> max_score = 4\n>>> score = 0\n>>> conditions = [plot_1.axes.get_xlabel() == 'x', plot_1.axes.get_ylabel() == 'y']\n>>> statements = ['xlabel is incorrect', 'ylabel is incorrect']\n>>> for condition in conditions:\n...     if condition:\n...         score += 2\n>>> drexel_jupyter_logger.variable_logger_csv(f'{score}, {max_score}', question_id)\n>>> response = {'question_id': question_id, 'score': score, 'max_score': max_score, 'student_response': str([plot_1.axes.get_xlabel(), plot_1.axes.get_ylabel()])}\n>>> responder.add_response(response)\n>>> for (condition, statement) in zip(conditions, statements):\n...     assert condition, statement\n",
         "failure_message": "The plot labels are incorrect.",
         "hidden": false,
         "locked": false,
         "points": 4,
         "success_message": "The plot labels are correct."
        },
        {
         "code": ">>> import drexel_jupyter_logger\n>>> from ENGR131_Util_2024 import submit_score, get_string_from_encrypted_string\n>>> from unittest.mock import patch\n>>> import json\n>>> from ENGR131_Util_2024 import responses, ResponseStore\n>>> responder = ResponseStore()\n>>> if 'drexel_email' not in responses:\n...     raise ValueError('Please fill out the student info form and run the test again')\n>>> scorer = submit_score()\n>>> question_id = 'q9_9'\n>>> max_score = 4\n>>> score = 0\n>>> conditions = [plot_1.get_label() == 'Noisy Data', plot_2[0].get_label() == 'Fitted Gaussian']\n>>> statements = ['Noisy Data', 'plot_1 is not labeled correctly', 'Fitted Gaussian', 'plot_2 is not labeled correctly']\n>>> for condition in conditions:\n...     if condition:\n...         score += 2\n>>> drexel_jupyter_logger.variable_logger_csv(f'{score}, {max_score}', question_id)\n>>> response = {'question_id': question_id, 'score': score, 'max_score': max_score, 'student_response': str([plot_1.get_label(), plot_2[0].get_label()])}\n>>> responder.add_response(response)\n>>> for (condition, statement) in zip(conditions, statements):\n...     assert condition, statement\n",
         "failure_message": "the plots are not labeled correctly",
         "hidden": false,
         "locked": false,
         "points": 1,
         "success_message": "the plots are labeled correctly"
        },
        {
         "code": ">>> import drexel_jupyter_logger\n>>> from ENGR131_Util_2024 import submit_score, get_string_from_encrypted_string\n>>> from unittest.mock import patch\n>>> import json\n>>> from ENGR131_Util_2024 import responses, ResponseStore\n>>> responder = ResponseStore()\n>>> if 'drexel_email' not in responses:\n...     raise ValueError('Please fill out the student info form and run the test again')\n>>> scorer = submit_score()\n>>> question_id = 'q9_10'\n>>> max_score = 3\n>>> score = 0\n>>> conditions = [plot_2[0].get_color() == (0.0, 0.5, 0.0, 1) or plot_2[0].get_color() == 'g' or plot_2[0].get_color() == 'green']\n>>> statements = ['gaussian fit is not the correct color']\n>>> for condition in conditions:\n...     if condition:\n...         score += 3\n>>> drexel_jupyter_logger.variable_logger_csv(f'{score}, {max_score}', question_id)\n>>> response = {'question_id': question_id, 'score': score, 'max_score': max_score, 'student_response': str([plot_2[0].get_color()])}\n>>> responder.add_response(response)\n>>> for (condition, statement) in zip(conditions, statements):\n...     assert condition, statement\n",
         "failure_message": "gaussian fit is not the correct color",
         "hidden": false,
         "locked": false,
         "points": 3,
         "success_message": "gaussian fit is the correct color"
        },
        {
         "code": ">>> import drexel_jupyter_logger\n>>> from ENGR131_Util_2024 import submit_score, get_string_from_encrypted_string\n>>> from unittest.mock import patch\n>>> import json\n>>> from ENGR131_Util_2024 import responses, ResponseStore\n>>> responder = ResponseStore()\n>>> if 'drexel_email' not in responses:\n...     raise ValueError('Please fill out the student info form and run the test again')\n>>> scorer = submit_score()\n>>> question_id = 'q9_11'\n>>> max_score = 3\n>>> score = 0\n>>> conditions = [plot_1.axes.get_legend() is not None]\n>>> statements = ['The legend is not correct on the plot']\n>>> for condition in conditions:\n...     if condition:\n...         score += 3\n>>> drexel_jupyter_logger.variable_logger_csv(f'{score}, {max_score}', question_id)\n>>> response = {'question_id': question_id, 'score': score, 'max_score': max_score, 'student_response': str([plot_1.axes.get_legend() is not None])}\n>>> responder.add_response(response)\n>>> for (condition, statement) in zip(conditions, statements):\n...     assert condition, statement\n",
         "failure_message": "The legend is not correct on the plot",
         "hidden": false,
         "locked": false,
         "points": 3,
         "success_message": "The legend is correct on the plot"
        },
        {
         "code": ">>> import drexel_jupyter_logger\n>>> from ENGR131_Util_2024 import submit_score, get_string_from_encrypted_string\n>>> from unittest.mock import patch\n>>> import json\n>>> from ENGR131_Util_2024 import responses, ResponseStore\n>>> responder = ResponseStore()\n>>> if 'drexel_email' not in responses:\n...     raise ValueError('Please fill out the student info form and run the test again')\n>>> scorer = submit_score()\n>>> question_id = 'q9_12'\n>>> max_score = 6\n>>> score = 0\n>>> conditions = [all(plot_2[0].get_xdata() == x_data), all(plot_2[0].get_ydata() == y_fit)]\n>>> statements = ['the x data for plot_2 is not correct', 'the y data for plot_2 is not correct']\n>>> for condition in conditions:\n...     if condition:\n...         score += 3\n>>> drexel_jupyter_logger.variable_logger_csv(f'{score}, {max_score}', question_id)\n>>> response = {'question_id': question_id, 'score': score, 'max_score': max_score, 'student_response': str(conditions)}\n>>> responder.add_response(response)\n>>> for (condition, statement) in zip(conditions, statements):\n...     assert condition, statement\n",
         "failure_message": "plot_2 does not have the correct values.",
         "hidden": false,
         "locked": false,
         "points": 6,
         "success_message": "plot_2 is correct."
        },
        {
         "code": ">>> import drexel_jupyter_logger\n>>> from ENGR131_Util_2024 import submit_score, get_string_from_encrypted_string\n>>> from unittest.mock import patch\n>>> import json\n>>> from ENGR131_Util_2024 import responses, ResponseStore\n>>> responder = ResponseStore()\n>>> if 'drexel_email' not in responses:\n...     raise ValueError('Please fill out the student info form and run the test again')\n>>> scorer = submit_score()\n>>> question_id = 'q9_13'\n>>> max_score = 6\n>>> score = 0\n>>> conditions = [np.array_equal(plot_1.get_offsets()[:, 1], y_noisy), np.array_equal(plot_1.get_offsets()[:, 0], x_data)]\n>>> statements = ['the y data for plot_1 is not correct', 'the x data for plot_1 is not correct']\n>>> for condition in conditions:\n...     if condition:\n...         score += 3\n>>> drexel_jupyter_logger.variable_logger_csv(f'{score}, {max_score}', question_id)\n>>> response = {'question_id': question_id, 'score': score, 'max_score': max_score, 'student_response': str(conditions)}\n>>> responder.add_response(response)\n>>> for (condition, statement) in zip(conditions, statements):\n...     assert condition, statement\n",
         "failure_message": "plot_1 does not have the correct values.",
         "hidden": false,
         "locked": false,
         "points": 6,
         "success_message": "plot_1 is correct."
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "d7d046052154998ca7dd3d9af52f7220fee50748c9a05b256540159ca8eb430c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}